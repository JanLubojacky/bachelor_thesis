{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZZMzWuVjwub"
      },
      "source": [
        "## Notebook for neural networks training and experiments\n",
        "\n",
        "For the purposes of bachelor thesis of Jan Lubojacký, Software application for real time analysis of ECG signal on AI chip, 2022 at CTU FEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5G8-LCkjsmD"
      },
      "source": [
        "Installs the necessary modules and known working versions of some libraries, other versions caused some troubles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OtraDEjWsyoj",
        "outputId": "db691186-aacd-455c-d82a-678210ec5f71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas==1.3.5 in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.5) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.5) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.5) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.5) (1.15.0)\n",
            "Collecting pickle5\n",
            "  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n",
            "\u001b[K     |████████████████████████████████| 256 kB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.12\n",
            "Collecting tensorflow==2.7\n",
            "  Downloading tensorflow-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 489.6 MB 24 kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (3.1.0)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 34.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (4.2.0)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (1.14.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (3.17.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (0.37.1)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (14.0.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (1.21.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (1.44.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (1.0.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (0.25.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (1.15.0)\n",
            "Collecting gast<0.5.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (1.6.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7) (3.2.0)\n",
            "Installing collected packages: tensorflow-estimator, keras, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "Successfully installed gast-0.4.0 keras-2.7.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "keras",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neurokit2\n",
            "  Downloading neurokit2-0.1.7-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from neurokit2) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (3.0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->neurokit2) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->neurokit2) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->neurokit2) (2022.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->neurokit2) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->neurokit2) (3.1.0)\n",
            "Installing collected packages: neurokit2\n",
            "Successfully installed neurokit2-0.1.7\n",
            "\u001b[K     |████████████████████████████████| 237 kB 5.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pandas==1.3.5\n",
        "!pip install pickle5\n",
        "!pip install tensorflow==2.7\n",
        "!pip install neurokit2\n",
        "!pip install -q tensorflow-model-optimization\n",
        "# this requires the mobilenet.py to be placed in google drive\n",
        "# at the specified path, alternatively it can be uploaded directly\n",
        "# into the session\n",
        "!cp /content/drive/MyDrive/bakalarka/mobilenet.py . "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qNybdtbkg4p"
      },
      "source": [
        "**Imports the necessary modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toPl4rIlQBTH",
        "outputId": "a2045e8d-cd40-4d6e-bd93-a06c1544ec4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n",
            "1.3.5\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import math\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, Dropout, MaxPooling1D, Flatten, Dense, GlobalAveragePooling1D, SeparableConv1D\n",
        "from keras.layers import BatchNormalization, Normalization, Activation, GlobalMaxPooling1D\n",
        "from keras.layers.pooling import AveragePooling1D\n",
        "from sklearn.utils import shuffle\n",
        "import pickle5 as pickle\n",
        "import pandas as pd\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "import os\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import mobilenet\n",
        "import neurokit2 as nk\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "import seaborn as sn\n",
        "from pprint import pprint\n",
        "\n",
        "# pandas 1.3.x recommended, lower version does not support the protocol\n",
        "# needed to unpickle the files\n",
        "# I use tf version 2.7, because 2.8 caused issues with quantization\n",
        "\n",
        "print(tf.__version__)\n",
        "print(pd.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnTwVsSHP2sp"
      },
      "source": [
        "**Load datasets and preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SodSy3bWs4Yh",
        "outputId": "54300697-b00c-4ede-896d-691360774d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['BIOMECH_5s_500Hz_prepro_False.pkl', 'WESAD_5s_500Hz_prepro_False.pkl', 'CLAS_5s_500Hz_prepro_False.pkl']\n",
            "BIOMECH_5s_500Hz_prepro_False.pkl <class 'pandas.core.frame.DataFrame'>\n",
            "WESAD_5s_500Hz_prepro_False.pkl <class 'pandas.core.frame.DataFrame'>\n",
            "CLAS_5s_500Hz_prepro_False.pkl <class 'pandas.core.frame.DataFrame'>\n",
            "EQUALIZING CLASSES\n",
            "unbalanced:  {0: 3996, 1: 5270}\n",
            "balanced:  {0: 3996, 1: 3996}\n",
            "                                                    ECG  label\n",
            "0     [0.521962, 0.535427, 0.601648, 0.623433, 0.546...      0\n",
            "1     [0.492684, 0.476485, 0.392296, 0.343058, 0.398...      0\n",
            "2     [0.497291, 0.512911, 0.60925, 0.711345, 0.6837...      0\n",
            "3     [0.501066, 0.491496, 0.438298, 0.397938, 0.423...      0\n",
            "4     [0.517815, 0.511725, 0.479121, 0.462063, 0.491...      0\n",
            "...                                                 ...    ...\n",
            "7987  [0.33393, 0.366622, 0.515114, 0.566868, 0.5969...      1\n",
            "7988  [0.488592, 0.544558, 0.840425, 0.990878, 0.760...      1\n",
            "7989  [0.480942, 0.469744, 0.410711, 0.374196, 0.418...      1\n",
            "7990  [0.419739, 0.469058, 0.669828, 0.486581, 0.0, ...      1\n",
            "7991  [0.436382, 0.455909, 0.460886, 0.220039, 0.134...      1\n",
            "\n",
            "[7992 rows x 2 columns]\n",
            "train classes\n",
            "{0: 2424, 1: 2371}\n",
            "val classes\n",
            "{0: 797, 1: 801}\n",
            "test classes\n",
            "{0: 775, 1: 824}\n",
            "input_length 500\n"
          ]
        }
      ],
      "source": [
        "data = pd.DataFrame()\n",
        "\n",
        "datasets_path = os.path.join('drive','MyDrive','bakalarka','datasets','5s_500Hz_Falsefilt_new')\n",
        "datasets = os.listdir(datasets_path)\n",
        "print(datasets)\n",
        "\n",
        "def equalize_classes(data, random_state):\n",
        "  print(\"EQUALIZING CLASSES\")\n",
        "\n",
        "  unique, counts = np.unique(data['label'], return_counts=True)\n",
        "  print('unbalanced: ', dict(zip(unique, counts)))\n",
        "\n",
        "  data_negative_class = data.loc[data['label'] == 0]\n",
        "  data_positive_class = data.loc[data['label'] == 1]\n",
        "\n",
        "  if len(data_positive_class) > len(data_negative_class):\n",
        "    data_positive_class = data_positive_class.sample(n = len(data_negative_class), random_state = random_state)\n",
        "  elif len(data_positive_class) < len(data_negative_class):\n",
        "    data_negative_class = data_negative_class.sample(n = len(data_positive_class), random_state = random_state)\n",
        "    \n",
        "  data = pd.concat([data_negative_class, data_positive_class], ignore_index=True)\n",
        "\n",
        "  unique, counts = np.unique(data['label'], return_counts=True)\n",
        "  print('balanced: ', dict(zip(unique, counts)))\n",
        "\n",
        "  return data\n",
        "\n",
        "def scale_data(data):\n",
        "\n",
        "  print(\"SCALING DATA\")\n",
        "  \n",
        "  for i in range(len(data['ECG'])):\n",
        "    arr = data['ECG'][i]\n",
        "\n",
        "    arr = (arr - np.min(arr)) / (np.max(arr) - np.min(arr))\n",
        "\n",
        "    data.at[i, 'ECG'] = arr\n",
        "\n",
        "  return data\n",
        "\n",
        "def standardize_data(data):\n",
        "\n",
        "  print(\"STANDARDIZING DATA\")\n",
        "\n",
        "  for i in range(len(data['ECG'])):\n",
        "      \n",
        "    arr = data['ECG'][i]\n",
        "\n",
        "    arr = (arr - np.mean(arr) / np.std(arr))\n",
        "\n",
        "    data.at[i, 'ECG'] = arr\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "'''\n",
        "load datasets\n",
        "'''\n",
        "\n",
        "for dataset in datasets:\n",
        "  # uncommenting the coresponidng line excludes the dataset from the training data\n",
        "\n",
        "  # if \"CLAS\" in dataset: continue\n",
        "  # if \"WESAD\" in dataset: continue\n",
        "  # if \"BIOMECH\" in dataset: continue\n",
        "  dataset_path = os.path.join(datasets_path,dataset)\n",
        "\n",
        "  with open(dataset_path, 'rb') as handle:\n",
        "    data_part = pickle.load(handle)\n",
        "    data = pd.concat([data,data_part], ignore_index=True)\n",
        "\n",
        "    print(dataset, type(data_part))\n",
        "\n",
        "'''\n",
        "data inspection\n",
        "'''\n",
        "\n",
        "# scale data, it is here because I wanted to have the ability\n",
        "# to experiment with unnormalized/normalized data and this way\n",
        "# there is no need to create extra datasets for it\n",
        "\n",
        "data = equalize_classes(data, random_state = 22) # handle class imbalances\n",
        "\n",
        "# data = standardize_data(data) # or standardize the data\n",
        "\n",
        "data_path = os.path.join('drive','MyDrive','bakalarka','datasets','ecg_folder.pkl')\n",
        "\n",
        "with open(data_path, 'rb') as handle:\n",
        "    ecg_data = pickle.load(handle)\n",
        "\n",
        "ecg_data = ecg_data.to_numpy()\n",
        "ecg_data = ecg_data.flatten()\n",
        "ecg_data = np.array_split(ecg_data,7992)\n",
        "data['ECG'] = ecg_data\n",
        "\n",
        "print(data)\n",
        "\n",
        "'''\n",
        "Ratios\n",
        "'''\n",
        "train_ratio = 0.6\n",
        "val_ratio = 0.2     # Percentage of samples that should be held for validation set\n",
        "test_ratio = 1 - (train_ratio + val_ratio) # Percentage of samples that should be held for test set\n",
        "\n",
        "'''\n",
        "Train, val, test splits\n",
        "'''\n",
        "\n",
        "train, val, test = \\\n",
        "              np.split(data.sample(frac=1, random_state=24), \n",
        "                       [int(train_ratio*len(data)), int((1-val_ratio)*len(data))])\n",
        "              \n",
        "x_train = np.stack(train['ECG'].values, axis=0)\n",
        "y_train = np.stack(train['label'].values, axis=0)\n",
        "\n",
        "x_val = np.stack(val['ECG'].values, axis=0)\n",
        "y_val = np.stack(val['label'].values, axis=0)\n",
        "\n",
        "x_test = np.stack(test['ECG'].values, axis=0)\n",
        "y_test = np.stack(test['label'].values, axis=0)\n",
        "\n",
        "df_test = {\n",
        "    'ECG' : x_test,\n",
        "    'label' : y_test\n",
        "}\n",
        "\n",
        "with open(''+'test_data.pkl', 'wb') as handle:\n",
        "    pickle.dump(df_test, handle, protocol=4)\n",
        "\n",
        "# validate, that there is aprox. equal amount of each class in each split\n",
        "unique, counts = np.unique(train['label'], return_counts=True)\n",
        "print('train classes')\n",
        "print(dict(zip(unique, counts)))\n",
        "\n",
        "unique, counts = np.unique(val['label'], return_counts=True)\n",
        "print('val classes')\n",
        "print(dict(zip(unique, counts)))\n",
        "\n",
        "unique, counts = np.unique(test['label'], return_counts=True)\n",
        "print('test classes')\n",
        "print(dict(zip(unique, counts)))\n",
        "\n",
        "# input length for neural networks\n",
        "input_length = data['ECG'][0].size\n",
        "\n",
        "print('input_length', input_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjLt115u3K6f"
      },
      "source": [
        "**Cross validation for optimal treshold, and also comparison between the tresholding method and a more advanced one**\n",
        "\n",
        "optimal treshold is aproximately 0.677 for neurokit preprocessing\n",
        "and 0.713 for microcontroller preprocessing\n",
        "\n",
        "The cell takes about a 1m20s to run, the neurokit R peak detection is very slow compared to the tresholding method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ9AK0-XiPNk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Testing R peak detection\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SMALL_SIZE = 16\n",
        "MEDIUM_SIZE = 20\n",
        "BIGGER_SIZE = 12\n",
        "\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
        "\n",
        "def heart_rate(mean_rr_interval, sampling_rate):\n",
        "  return 60 * (sampling_rate / mean_rr_interval)\n",
        "\n",
        "def mean_rr_interval(peaks_sample_num):\n",
        "  return np.mean(np.diff(peaks_sample_num))\n",
        "\n",
        "def get_signal_parameters_nk(x,y):\n",
        "  parameters_x = []\n",
        "  parameters_y = []\n",
        "  errors = 0\n",
        "\n",
        "  for i in range(len(x)):\n",
        "    signal = x[i]\n",
        "\n",
        "    signals, info = nk.ecg_peaks(signal, correct_artifacts=True, method = 'neurokit', sampling_rate = 100)\n",
        "\n",
        "    unique, counts = np.unique(signals['ECG_R_Peaks'].values, return_counts=True)\n",
        "\n",
        "    peaks_sample_number = np.where(signals['ECG_R_Peaks'].values==1)[0]\n",
        "\n",
        "    mean_rr = mean_rr_interval(peaks_sample_number)\n",
        "    hr = heart_rate(mean_rr, 100)\n",
        "\n",
        "    parameters_x.append(peaks_sample_number)\n",
        "    parameters_y.append(hr)\n",
        "\n",
        "  return np.array(parameters_x), np.array(parameters_y)\n",
        "\n",
        "\n",
        "def get_signal_parameters(x, y, repolarization_window=40, treshold=0.677):\n",
        "\n",
        "  parameters_x = []\n",
        "  parameters_y = []\n",
        "  errors = 0\n",
        "\n",
        "  for i in range(len(x)):\n",
        "    signal = x[i]\n",
        "    label = y[i]\n",
        "\n",
        "    peaks = np.zeros_like(signal)\n",
        "\n",
        "    peaks_sample_number = []\n",
        "    peak_count = 0\n",
        "    last_signal_peak = False\n",
        "    repolarization = False\n",
        "    repolarization_count = repolarization_window\n",
        "\n",
        "    for i in range(len(signal)):\n",
        "\n",
        "      if repolarization == True:\n",
        "        repolarization_count -= 1\n",
        "\n",
        "        if repolarization_count == 0:\n",
        "          repolarization = False\n",
        "          repolarization_count = repolarization_window\n",
        "\n",
        "      elif repolarization == False:\n",
        "\n",
        "        if signal[i] > treshold and last_signal_peak == False:\n",
        "          last_signal_peak = True\n",
        "          peaks[i] = 1\n",
        "          peak_count += 1\n",
        "          peaks_sample_number.append(i)\n",
        "          repolarization = True\n",
        "        else:\n",
        "          last_signal_peak = False\n",
        "\n",
        "    mean_rr = mean_rr_interval(peaks_sample_number)\n",
        "    hr = heart_rate(mean_rr, 100)\n",
        "\n",
        "    parameters_x.append(peaks_sample_number)\n",
        "    parameters_y.append(hr)\n",
        "\n",
        "  return np.array(parameters_x), np.array(parameters_y)\n",
        "\n",
        "tresholds = np.linspace(0.6, 0.9, num=30)\n",
        "\n",
        "repolariztaion_window = np.arange(30, 60, 1)\n",
        "\n",
        "cross_val_array = tresholds # or repolariztaion_window\n",
        "\n",
        "positions_nk, hr_nk = get_signal_parameters_nk(x_train, y_train)\n",
        "\n",
        "for win in cross_val_array:\n",
        "\n",
        "  positions_treshold, hr_treshold = get_signal_parameters(x_train, y_train, treshold=win)\n",
        "\n",
        "  bigger_diff = 0\n",
        "  correct = 0\n",
        "  one_beat_diff = 0\n",
        "  two_beat_diff = 0\n",
        "\n",
        "  print(len(hr_nk), len(hr_treshold))\n",
        "\n",
        "  difference = abs(hr_nk - hr_treshold)\n",
        "  tr = np.where(difference <= 5)[0]\n",
        "\n",
        "  print(len(tr))\n",
        "\n",
        "  print(len(positions_treshold))\n",
        "\n",
        "  for i in range(len(positions_treshold)):\n",
        "    if len(positions_treshold[i]) == len(positions_nk[i]):\n",
        "\n",
        "      differences = abs(positions_treshold[i] - positions_nk[i])\n",
        "\n",
        "      if differences.all() < 5:\n",
        "        correct += 1\n",
        "\n",
        "      # if correct % 2000 == 0:\n",
        "      #   x_values = np.linspace(start=0,stop=5,num=500)\n",
        "      #   plt.figure(1, figsize=(15, 10), dpi=80)\n",
        "      #   plt.plot(x_values, x_train[i])\n",
        "      #   peaks_det = np.take(x_values, positions_treshold[i])\n",
        "      #   plt.plot(peaks_det, np.ones_like(peaks_det), 'bo', color='red')\n",
        "      #   peaks_det_nk = np.take(x_values, positions_nk[i])\n",
        "      #   plt.plot(peaks_det_nk, np.ones_like(peaks_det_nk), 'bo', color='green')\n",
        "      #   plt.xlabel(\"time[s]\")\n",
        "      #   plt.ylabel(\"ECG [-]\")\n",
        "\n",
        "    elif len(positions_treshold[i]) - 1 == len(positions_nk[i]):\n",
        "\n",
        "      pos1 = abs(positions_treshold[i][:-1] - positions_nk[i])\n",
        "      pos2 = abs(positions_treshold[i][1:] - positions_nk[i])\n",
        "\n",
        "      if (pos1.all() < 15):\n",
        "        one_beat_diff += 1\n",
        "      elif (pos2.all() < 15):\n",
        "        one_beat_diff += 1\n",
        "\n",
        "    elif len(positions_nk[i]) - 1 == len(positions_treshold[i]):\n",
        "\n",
        "      pos1 = abs(positions_nk[i][:-1] - positions_treshold[i])\n",
        "      pos2 = abs(positions_nk[i][1:] - positions_treshold[i])\n",
        "\n",
        "      if (pos1.all() < 15):\n",
        "        one_beat_diff += 1\n",
        "      elif (pos2.all() < 15):\n",
        "        one_beat_diff += 1\n",
        "\n",
        "    elif len(positions_treshold[i]) - 2 == len(positions_nk[i]):\n",
        "\n",
        "      pos1 = abs(positions_treshold[i][1:-1] - positions_nk[i])\n",
        "\n",
        "      if (pos1.all() < 15):\n",
        "        two_beat_diff += 1\n",
        "\n",
        "    else:\n",
        "      bigger_diff += 1\n",
        "\n",
        "      # if bigger_diff % 50 == 0:\n",
        "      #   x_values = np.linspace(start=0,stop=5,num=500)\n",
        "      #   plt.figure(1, figsize=(15, 10), dpi=80)\n",
        "      #   plt.plot(x_values, x_train[i])\n",
        "      #   peaks_det = np.take(x_values, positions_treshold[i])\n",
        "      #   plt.plot(peaks_det, np.ones_like(peaks_det), 'bo', color='red')\n",
        "      #   peaks_det_nk = np.take(x_values, positions_nk[i])\n",
        "      #   plt.plot(peaks_det_nk, np.ones_like(peaks_det_nk), 'bo', color='green')\n",
        "      #   plt.xlabel(\"time[s]\")\n",
        "      #   plt.ylabel(\"ECG [-]\")\n",
        "\n",
        "  print('treshold', win, 'correct', correct, 'one_beat_diff', one_beat_diff, 'two_beat_diff', two_beat_diff, 'bigger_diff', bigger_diff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URqeNHqTlKBq"
      },
      "source": [
        "**Models declarations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRlCD3YgGhcy",
        "outputId": "1b245479-d820-4d89-d3f4-de0af6af519a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"MobileNetv3modded\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 500, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " conv1d_22 (Conv1D)             (None, 250, 8)       32          ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 250, 8)      32          ['conv1d_22[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_18 (TFOpL  (None, 250, 8)      0           ['batch_normalization_28[0][0]'] \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.nn.relu6_22 (TFOpLambda)    (None, 250, 8)       0           ['tf.__operators__.add_18[0][0]']\n",
            "                                                                                                  \n",
            " tf.math.multiply_18 (TFOpLambd  (None, 250, 8)      0           ['batch_normalization_28[0][0]', \n",
            " a)                                                               'tf.nn.relu6_22[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.truediv_18 (TFOpLambda  (None, 250, 8)      0           ['tf.math.multiply_18[0][0]']    \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_23 (Conv1D)             (None, 250, 8)       72          ['tf.math.truediv_18[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 250, 8)      32          ['conv1d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.nn.relu6_23 (TFOpLambda)    (None, 250, 8)       0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " separable_conv1d_8 (SeparableC  (None, 125, 8)      96          ['tf.nn.relu6_23[0][0]']         \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 125, 8)      32          ['separable_conv1d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.nn.relu6_24 (TFOpLambda)    (None, 125, 8)       0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " global_average_pooling1d_6 (Gl  (None, 8)           0           ['tf.nn.relu6_24[0][0]']         \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 8)            72          ['global_average_pooling1d_6[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 8)            72          ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " reshape_4 (Reshape)            (None, 1, 8)         0           ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 125, 8)       0           ['tf.nn.relu6_24[0][0]',         \n",
            "                                                                  'reshape_4[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_24 (Conv1D)             (None, 125, 8)       72          ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 125, 8)      32          ['conv1d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_25 (Conv1D)             (None, 125, 16)      144         ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 125, 16)     64          ['conv1d_25[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_19 (TFOpL  (None, 125, 16)     0           ['batch_normalization_32[0][0]'] \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.nn.relu6_25 (TFOpLambda)    (None, 125, 16)      0           ['tf.__operators__.add_19[0][0]']\n",
            "                                                                                                  \n",
            " tf.math.multiply_19 (TFOpLambd  (None, 125, 16)     0           ['batch_normalization_32[0][0]', \n",
            " a)                                                               'tf.nn.relu6_25[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.truediv_19 (TFOpLambda  (None, 125, 16)     0           ['tf.math.multiply_19[0][0]']    \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " separable_conv1d_9 (SeparableC  (None, 63, 10)      218         ['tf.math.truediv_19[0][0]']     \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 63, 10)      40          ['separable_conv1d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_20 (TFOpL  (None, 63, 10)      0           ['batch_normalization_33[0][0]'] \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.nn.relu6_26 (TFOpLambda)    (None, 63, 10)       0           ['tf.__operators__.add_20[0][0]']\n",
            "                                                                                                  \n",
            " tf.math.multiply_20 (TFOpLambd  (None, 63, 10)      0           ['batch_normalization_33[0][0]', \n",
            " a)                                                               'tf.nn.relu6_26[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.truediv_20 (TFOpLambda  (None, 63, 10)      0           ['tf.math.multiply_20[0][0]']    \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_26 (Conv1D)             (None, 63, 10)       110         ['tf.math.truediv_20[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 63, 10)      40          ['conv1d_26[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_27 (Conv1D)             (None, 63, 32)       352         ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 63, 32)      128         ['conv1d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_21 (TFOpL  (None, 63, 32)      0           ['batch_normalization_35[0][0]'] \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.nn.relu6_27 (TFOpLambda)    (None, 63, 32)       0           ['tf.__operators__.add_21[0][0]']\n",
            "                                                                                                  \n",
            " tf.math.multiply_21 (TFOpLambd  (None, 63, 32)      0           ['batch_normalization_35[0][0]', \n",
            " a)                                                               'tf.nn.relu6_27[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.truediv_21 (TFOpLambda  (None, 63, 32)      0           ['tf.math.multiply_21[0][0]']    \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " separable_conv1d_10 (Separable  (None, 63, 10)      426         ['tf.math.truediv_21[0][0]']     \n",
            " Conv1D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 63, 10)      40          ['separable_conv1d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_22 (TFOpL  (None, 63, 10)      0           ['batch_normalization_36[0][0]'] \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.nn.relu6_28 (TFOpLambda)    (None, 63, 10)       0           ['tf.__operators__.add_22[0][0]']\n",
            "                                                                                                  \n",
            " tf.math.multiply_22 (TFOpLambd  (None, 63, 10)      0           ['batch_normalization_36[0][0]', \n",
            " a)                                                               'tf.nn.relu6_28[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.truediv_22 (TFOpLambda  (None, 63, 10)      0           ['tf.math.multiply_22[0][0]']    \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_28 (Conv1D)             (None, 63, 10)       110         ['tf.math.truediv_22[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 63, 10)      40          ['conv1d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 63, 10)       0           ['batch_normalization_37[0][0]', \n",
            "                                                                  'batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_29 (Conv1D)             (None, 63, 64)       704         ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 63, 64)      256         ['conv1d_29[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_23 (TFOpL  (None, 63, 64)      0           ['batch_normalization_38[0][0]'] \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.nn.relu6_29 (TFOpLambda)    (None, 63, 64)       0           ['tf.__operators__.add_23[0][0]']\n",
            "                                                                                                  \n",
            " tf.math.multiply_23 (TFOpLambd  (None, 63, 64)      0           ['batch_normalization_38[0][0]', \n",
            " a)                                                               'tf.nn.relu6_29[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.truediv_23 (TFOpLambda  (None, 63, 64)      0           ['tf.math.multiply_23[0][0]']    \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " separable_conv1d_11 (Separable  (None, 63, 12)      1100        ['tf.math.truediv_23[0][0]']     \n",
            " Conv1D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 63, 12)      48          ['separable_conv1d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_24 (TFOpL  (None, 63, 12)      0           ['batch_normalization_39[0][0]'] \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.nn.relu6_30 (TFOpLambda)    (None, 63, 12)       0           ['tf.__operators__.add_24[0][0]']\n",
            "                                                                                                  \n",
            " tf.math.multiply_24 (TFOpLambd  (None, 63, 12)      0           ['batch_normalization_39[0][0]', \n",
            " a)                                                               'tf.nn.relu6_30[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.truediv_24 (TFOpLambda  (None, 63, 12)      0           ['tf.math.multiply_24[0][0]']    \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_7 (Gl  (None, 12)          0           ['tf.math.truediv_24[0][0]']     \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 12)           156         ['global_average_pooling1d_7[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 12)           156         ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " reshape_5 (Reshape)            (None, 1, 12)        0           ['dense_13[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 63, 12)       0           ['tf.math.truediv_24[0][0]',     \n",
            "                                                                  'reshape_5[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_30 (Conv1D)             (None, 63, 12)       156         ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 63, 12)      48          ['conv1d_30[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_31 (Conv1D)             (None, 63, 5)        65          ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 63, 5)       20          ['conv1d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_25 (TFOpL  (None, 63, 5)       0           ['batch_normalization_41[0][0]'] \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.nn.relu6_31 (TFOpLambda)    (None, 63, 5)        0           ['tf.__operators__.add_25[0][0]']\n",
            "                                                                                                  \n",
            " tf.math.multiply_25 (TFOpLambd  (None, 63, 5)       0           ['batch_normalization_41[0][0]', \n",
            " a)                                                               'tf.nn.relu6_31[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.truediv_25 (TFOpLambda  (None, 63, 5)       0           ['tf.math.multiply_25[0][0]']    \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_26 (TFOpL  (None, 63, 5)       0           ['tf.math.truediv_25[0][0]']     \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.nn.relu6_32 (TFOpLambda)    (None, 63, 5)        0           ['tf.__operators__.add_26[0][0]']\n",
            "                                                                                                  \n",
            " tf.math.multiply_26 (TFOpLambd  (None, 63, 5)       0           ['tf.math.truediv_25[0][0]',     \n",
            " a)                                                               'tf.nn.relu6_32[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.truediv_26 (TFOpLambda  (None, 63, 5)       0           ['tf.math.multiply_26[0][0]']    \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_32 (Conv1D)             (None, 63, 10)       60          ['tf.math.truediv_26[0][0]']     \n",
            "                                                                                                  \n",
            " global_average_pooling1d_8 (Gl  (None, 10)          0           ['conv1d_32[0][0]']              \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 10)           0           ['global_average_pooling1d_8[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 10)           0           ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 1)            11          ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,036\n",
            "Trainable params: 4,610\n",
            "Non-trainable params: 426\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def Parameters_model():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(tf.keras.Input(shape=(4,)))\n",
        "\n",
        "  model.add(Dense(20, activation = 'relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Dense(20, activation = 'relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Dense(20, activation = 'relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "  model._name = \"Parameters_model\"\n",
        "\n",
        "  return model\n",
        "\n",
        "def Custom_model(input_length):\n",
        "    model = Sequential()\n",
        "    model.add(\n",
        "        Conv1D(filters=8,\n",
        "               kernel_size=32,\n",
        "               strides=3,\n",
        "               input_shape=(input_length, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling1D(pool_size=3, strides=1))\n",
        "\n",
        "    model.add(SeparableConv1D(filters=16, kernel_size=24, strides=2))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(AveragePooling1D(pool_size=3, strides=1))\n",
        "\n",
        "    model.add(SeparableConv1D(filters=32, kernel_size=16, strides=1))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(AveragePooling1D(pool_size=2, strides=1))\n",
        "\n",
        "    model.add(SeparableConv1D(filters=64, kernel_size=4, strides=1))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(AveragePooling1D(pool_size=2, strides=1))\n",
        "    \n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model._name = \"Custom_model\"\n",
        "\n",
        "    return model\n",
        "\n",
        "def model_output(x, pooling, dropout_rate): \n",
        "  x = GlobalAveragePooling1D()(x)\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "  outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  return outputs\n",
        "\n",
        "def MobileNetv3modded(input_lenght, alpha, num_channel=1):\n",
        "  inputs = keras.Input(shape = (input_length,1))\n",
        "\n",
        "  x = mobilenet.Conv_1D_block_2(inputs, 8, 3, strides=2, nl='HS')\n",
        "\n",
        "  x = mobilenet.bottleneck_block_2(x, 8, 3, e=8, s=2, squeeze=True, nl='RE', alpha=alpha)\n",
        "  x = mobilenet.bottleneck_block_2(x, 10, 3, e=16, s=2, squeeze=False, nl='HS', alpha=alpha)\n",
        "  x = mobilenet.bottleneck_block_2(x, 10, 3, e=32, s=1, squeeze=False, nl='HS', alpha=alpha)\n",
        "  x = mobilenet.bottleneck_block_2(x, 12, 5, e=64, s=1, squeeze=True, nl='HS', alpha=alpha)\n",
        "\n",
        "  x = mobilenet.Conv_1D_block_2(x, 5, 1, strides=1, nl='HS')\n",
        "  x = x * tf.keras.activations.relu(x + 3.0, max_value=6.0) / 6.0\n",
        "  x = tf.keras.layers.Conv1D(10, 1, padding='same')(x)\n",
        "\n",
        "  outputs = model_output(x, pooling='avg', dropout_rate=0.1)\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  model._name = \"MobileNetv3modded\"\n",
        "\n",
        "  return model\n",
        "\n",
        "'''\n",
        "By uncomenting, choose the desired model for training\n",
        "'''\n",
        "\n",
        "model = MobileNetv3modded(input_length, 1)\n",
        "# model = Custom_model(input_length)\n",
        "# model = Parameters_model()\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3haKIRXlaDH"
      },
      "source": [
        "**lets find the optimal lr**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ip5Jke2kPeD"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "initial_history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4*10 ** (epoch / 30)))\n",
        "\n",
        "\n",
        "# Plot the training history\n",
        "loss = initial_history.history['loss']\n",
        "val_loss = initial_history.history['val_loss']\n",
        "\n",
        "train_acc = initial_history.history['accuracy']\n",
        "val_acc = initial_history.history['val_accuracy']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.plot(epochs, train_acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "914FOoCElg3N"
      },
      "source": [
        "**And lets plot the graphs from which we can determine what learning rate should we use**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DVngyPmn2Cb"
      },
      "outputs": [],
      "source": [
        "from matplotlib import rcParams\n",
        "\n",
        "rcParams['figure.figsize'] = (20, 16)\n",
        "rcParams['axes.spines.top'] = False\n",
        "rcParams['axes.spines.right'] = False \n",
        "\n",
        "\n",
        "plt.figure(1)\n",
        "plt.subplots_adjust(hspace = 0.5)\n",
        "plt.subplot(4,2,1)\n",
        "plt.plot(\n",
        "    np.arange(1, 101), \n",
        "    initial_history.history['loss'], \n",
        "    label='Loss', lw=3\n",
        ")\n",
        "plt.plot(\n",
        "    np.arange(1, 101), \n",
        "    initial_history.history['accuracy'], \n",
        "    label='Accuracy', lw=3\n",
        ")\n",
        "plt.plot(\n",
        "    np.arange(1, 101), \n",
        "    initial_history.history['lr'], \n",
        "    label='Learning rate', color='#000', lw=3, linestyle='--')\n",
        "plt.title('Evaluation metrics', size=16)\n",
        "plt.xlabel('Epoch', size=14)\n",
        "plt.legend();\n",
        "\n",
        "plt.figure(1)\n",
        "plt.subplot(4,2,2)\n",
        "plt.plot(\n",
        "    np.arange(1, 101), \n",
        "    initial_history.history['loss'], \n",
        "    label='Loss', lw=3\n",
        ")\n",
        "plt.plot(\n",
        "    np.arange(1, 101), \n",
        "    initial_history.history['val_accuracy'], \n",
        "    label='Validation Accuracy', lw=3\n",
        ")\n",
        "plt.plot(\n",
        "    np.arange(1, 101), \n",
        "    initial_history.history['lr'], \n",
        "    label='Learning rate', color='#000', lw=3, linestyle='--')\n",
        "plt.title('Evaluation metrics', size=16)\n",
        "plt.xlabel('Epoch', size=14)\n",
        "plt.legend();\n",
        "\n",
        "learning_rates = 1e-3 * (10 ** (np.arange(100) / 30))\n",
        "plt.subplot(4,2,3)\n",
        "plt.semilogx(\n",
        "    learning_rates, \n",
        "    initial_history.history['loss'], \n",
        "    lw=3, color='#000'\n",
        ")\n",
        "plt.title('Learning rate vs. loss', size=16)\n",
        "plt.xlabel('Learning rate', size=14)\n",
        "plt.ylabel('Loss', size=14);\n",
        "plt.subplot(4,2,4)\n",
        "plt.semilogx(\n",
        "    learning_rates, \n",
        "    initial_history.history['val_loss'], \n",
        "    lw=3, color='#000'\n",
        ")\n",
        "plt.title('Learning rate vs. val loss', size=16)\n",
        "plt.xlabel('Learning rate', size=14)\n",
        "plt.ylabel('Val loss', size=14);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgjqe8VFlr0A"
      },
      "source": [
        "**Training method for neural network, we can assign the chosen learning rate to the Adam optimizer. An early stopping callback is provided, this stop the training when the required accuracy on validation set is reached.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFWsY42S-O1d"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 400\n",
        "BATCH_SZ = 32\n",
        "new_lr = 0.0022\n",
        "\n",
        "class MyThresholdCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs.get('val_accuracy') >= 0.91:\n",
        "            self.model.stop_training = True\n",
        "\n",
        "\n",
        "# normal training\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "model.optimizer.lr.assign(new_lr)\n",
        "\n",
        "# Train model\n",
        "callbacks = [MyThresholdCallback()]\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    epochs=EPOCHS,\n",
        "                    batch_size=BATCH_SZ,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks = callbacks)\n",
        "\n",
        "# Plot the training history\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "train_acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training loss', color='orange', lw=3)\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, train_acc, 'b', label='Training accuracy', color='orange', lw=3)\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpXGHzSRmL9O"
      },
      "source": [
        "**This section runs the training with pruning callback**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpPwk8EMyQ5t"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SZ = 66\n",
        "new_lr =  0.0005\n",
        "\n",
        "# weigth pruning\n",
        "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model)\n",
        "callbacks = [ tfmot.sparsity.keras.UpdatePruningStep()]\n",
        "\n",
        "pruned_model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "pruned_model.optimizer.lr.assign(new_lr)\n",
        "\n",
        "history = pruned_model.fit(x_train,\n",
        "                    y_train,\n",
        "                    epochs=EPOCHS,\n",
        "                    batch_size=BATCH_SZ,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks = callbacks)\n",
        "\n",
        "model = tfmot.sparsity.keras.strip_pruning(pruned_model) \n",
        "\n",
        "# Plot the training history\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "train_acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training loss', color='orange', lw=3)\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, train_acc, 'b', label='Training accuracy', color='orange', lw=3)\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXJjgZxrmWLG"
      },
      "source": [
        "**Evaluation section, here are implemented all the evaluation methods**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "4vHFBfxlhCkH",
        "outputId": "4f35596f-b44c-4f08-edfb-f72c441a7ae2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'true_positives': 784, 'true_negatives': 696, 'false_positives': 79, 'false_negatives': 40, 'accuracy': 0.93, 'sensitivity': 0.95, 'specificity': 0.9, 'precision': 0.91, 'f1': 0.93}\n"
          ]
        }
      ],
      "source": [
        "def get_evaluation_metrics(model, x_test, y_test):\n",
        "\n",
        "    x_test = np.round(x_test, 6)\n",
        "    # print(x_test)\n",
        "    predictions = np.round(model.predict(x_test).flatten())\n",
        "\n",
        "    tn = 0\n",
        "    tp = 0\n",
        "    fn = 0\n",
        "    fp = 0\n",
        "\n",
        "    cmtx = confusion_matrix(y_test, predictions)\n",
        "    tn = cmtx[0][0]\n",
        "    fp = cmtx[0][1]\n",
        "    fn = cmtx[1][0]\n",
        "    tp = cmtx[1][1]\n",
        "\n",
        "    df_cm = pd.DataFrame(cmtx, range(2), range(2))\n",
        "    # plt.figure(figsize=(10,7))\n",
        "    sn.set(font_scale=1.4) # for label size\n",
        "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt = 'd') # font size\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    accuracy = (tn + tp) / (tn + tp + fn + fp)\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    precision = tp / (tp + fp)\n",
        "\n",
        "    f1 = f1_score(y_test, predictions)\n",
        "\n",
        "    eval_metrics = {\n",
        "        'true_positives': tp,\n",
        "        'true_negatives': tn,\n",
        "        'false_positives': fp,\n",
        "        'false_negatives': fn,\n",
        "        'accuracy': accuracy,\n",
        "        'sensitivity': sensitivity,\n",
        "        'specificity': specificity,\n",
        "        'precision': precision,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "    for key in eval_metrics:\n",
        "        # rounding to K using round()\n",
        "        eval_metrics[key] = round(eval_metrics[key], 2)\n",
        "\n",
        "    print(eval_metrics)\n",
        "\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_ecg in enumerate(x_test):\n",
        "    # print(test_ecg)\n",
        "    if i % 1000 == 0:\n",
        "      print('Evaluated on {n} results so far.'.format(n=i))\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_ecg = test_ecg.reshape(1,1000,1)\n",
        "    test_ecg = np.float32(test_ecg)\n",
        "    # print(test_ecg)\n",
        "    interpreter.set_tensor(input_index, test_ecg)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  print('\\n')\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == y_test).mean()\n",
        "  return accuracy\n",
        "  \n",
        "get_evaluation_metrics(model, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vEDmNbImnbm"
      },
      "source": [
        "**Here the model is converted with both the default conversion and the 8-bit quantization into a compressed tflite model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVQogKerDfm1",
        "outputId": "430a20d1-34a5-49c9-a49b-bc1252c850fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpk0lmvllt/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpk0lmvllt/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpjz2ssw9m/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpjz2ssw9m/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61440"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Convert Keras model to a tflite model\n",
        "# Dynamic range quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "tf_lite_default_opt = converter.convert()\n",
        "\n",
        "# Save the quantized file\n",
        "TFLITE_MODEL = 'mb3_100Hz_5s_ucfil_lowepoch' + 'uncompressed' + '.tflite'\n",
        "open(TFLITE_MODEL, 'wb').write(tf_lite_default_opt)\n",
        "\n",
        "# Convert Keras model to a tflite model\n",
        "# Full integer quantization\n",
        "def representative_dataset_gen():\n",
        "  for data in x_val:\n",
        "    sample = data.reshape(1,input_length,1)\n",
        "    sample = [np.float32(sample)]\n",
        "    yield sample\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "# This enables quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# This ensures that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "# For full integer quantization, though supported types defaults to int8 only\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "\n",
        "# These set the input and output tensors to float32, uint8 could be used, \n",
        "# however that decreased the accuracy greatly\n",
        "converter.inference_input_type = tf.float32  # or tf.int8/tf.float32\n",
        "converter.inference_output_type = tf.float32  # or tf.int8/tf.float32\n",
        "\n",
        "quant_model = converter.convert()\n",
        "\n",
        "# Save the quantized file\n",
        "TFLITE_QUANT_MODEL = 'mb3_100Hz_5s_ucfilt_lowepoch' + 'compressed' + '.tflite'\n",
        "open(TFLITE_QUANT_MODEL, 'wb').write(quant_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31oMOvhsnSAh"
      },
      "source": [
        "**Evaluation for both the compressed and uncompressed converted model to check whether the conversion decreased the models accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9j4I-ePQ8GS",
        "outputId": "6c62d593-3869-45e7-8a2c-e43aa47921e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BIOMECH_5s_100Hz_prepro_True.pkl\n",
            "drive/MyDrive/bakalarka/datasets/5s_100Hz_Truefilt_new/BIOMECH_5s_100Hz_prepro_True.pkl\n",
            "WESAD_5s_100Hz_prepro_True.pkl\n",
            "drive/MyDrive/bakalarka/datasets/5s_100Hz_Truefilt_new/WESAD_5s_100Hz_prepro_True.pkl\n",
            "CLAS_5s_100Hz_prepro_True.pkl\n",
            "drive/MyDrive/bakalarka/datasets/5s_100Hz_Truefilt_new/CLAS_5s_100Hz_prepro_True.pkl\n",
            "SCALING DATA\n",
            "mb3_100Hz_5s_ucfil_lowepochuncompressed.tflite\n",
            "{'true_positives': 3705, 'true_negatives': 996, 'false_positives': 3000, 'false_negatives': 146, 'accuracy': 0.599082451892443, 'sensitivity': 0.9620877694105427, 'specificity': 0.24924924924924924, 'precision': 0.5525727069351231, 'f1': 0.7019704433497538}\n"
          ]
        }
      ],
      "source": [
        "def tf_lite_model_eval(model_path, x_test, y_test):\n",
        "\n",
        "  print(model_path)\n",
        "\n",
        "  tflite_interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "\n",
        "  input_details = tflite_interpreter.get_input_details()\n",
        "  output_details = tflite_interpreter.get_output_details()\n",
        "\n",
        "  predictions = np.zeros(len(y_test))\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  input_length = len(x_test[0])\n",
        "\n",
        "  for ecg in x_test:\n",
        "    ecg = ecg.reshape(1,input_length,1)\n",
        "    ecg = np.float32(ecg)\n",
        "\n",
        "    tflite_interpreter.allocate_tensors()\n",
        "    tflite_interpreter.set_tensor(input_details[0]['index'], ecg)\n",
        "    tflite_interpreter.invoke()\n",
        "    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    predictions[i] = tflite_model_predictions\n",
        "    i+=1\n",
        "\n",
        "  predictions = np.round(predictions)\n",
        "\n",
        "  cmtx = confusion_matrix(y_test, predictions)\n",
        "  tn = cmtx[0][0]\n",
        "  fp = cmtx[0][1]\n",
        "  fn = cmtx[1][0]\n",
        "  tp = cmtx[1][1]\n",
        "\n",
        "  df_cm = pd.DataFrame(cmtx, range(2), range(2))\n",
        "  # plt.figure(figsize=(10,7))\n",
        "  sn.set(font_scale=1.4) # for label size\n",
        "  sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt = 'd') # font size\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  accuracy = (tn + tp) / (tn + tp + fn + fp)\n",
        "  sensitivity = tp / (tp + fn)\n",
        "  specificity = tn / (tn + fp)\n",
        "  precision = tp / (tp + fp)\n",
        "\n",
        "  f1 = f1_score(y_test, predictions)\n",
        "\n",
        "  return {\n",
        "      'true_positives': tp,\n",
        "      'true_negatives': tn,\n",
        "      'false_positives': fp,\n",
        "      'false_negatives': fn,\n",
        "      'accuracy': accuracy,\n",
        "      'sensitivity': sensitivity,\n",
        "      'specificity': specificity,\n",
        "      'precision': precision,\n",
        "      'f1': f1}\n",
        "\n",
        "def load_fresh_dataset(dataset_path):\n",
        "  \n",
        "  datasets = os.listdir(datasets_path)\n",
        "\n",
        "  compressed_test_data = pd.DataFrame()\n",
        "\n",
        "  for dataset in datasets:\n",
        "\n",
        "    # if \"CLAS\" in dataset: continue\n",
        "    # if \"WESAD\" in dataset: continue\n",
        "    # if \"BIOMECH\" in dataset: continue\n",
        "\n",
        "    print((dataset))\n",
        "\n",
        "    dataset_path = os.path.join(datasets_path,dataset)\n",
        "    \n",
        "    print(dataset_path)\n",
        "\n",
        "    with open(dataset_path, 'rb') as handle:\n",
        "      compressed_test_data_part = pickle.load(handle)\n",
        "\n",
        "      compressed_test_data = pd.concat([compressed_test_data, compressed_test_data_part], ignore_index=True)\n",
        "\n",
        "\n",
        "  compressed_test_data = scale_data(compressed_test_data)\n",
        "\n",
        "  x_compressed_test = np.stack(compressed_test_data['ECG'].values, axis=0)\n",
        "  y_compressed_test = np.stack(compressed_test_data['label'].values, axis=0)\n",
        "\n",
        "  # optionaly select model\n",
        "  # TFLITE_MODEL = '/content/drive/MyDrive/bakalarka/Tf_lite_models/cnn_100Hz_5s_94acc.tflite'\n",
        "  # TFLITE_QUANT_MODEL = '/content/drive/MyDrive/bakalarka/Tf_lite_models/cnn_100Hz_5s_ucfilt.tflite'\n",
        "\n",
        "  print(tf_lite_model_eval(TFLITE_MODEL, x_compressed_test, y_compressed_test))\n",
        "  print(tf_lite_model_eval(TFLITE_QUANT_MODEL, x_compressed_test, y_compressed_test))\n",
        "\n",
        "\n",
        "print(tf_lite_model_eval(TFLITE_MODEL, x_test, y_test))\n",
        "print(tf_lite_model_eval(TFLITE_QUANT_MODEL, x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZj978qWy8Ap"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ecg_nerual_net_prototype.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}